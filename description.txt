Для решения задачи понадобились два преобразования исходной RDD: 
1. map - для отображения входных строк "src\ttgt\tweight" на пары (node, weight)
2. reduceByKey - для суммирования весов по каждой вершине
Для представления полученной RDD в строковом виде используется map, переводящий двойки (node, totalWeight) в их строковые представления.

Наблюдения:
*Интересно отметить, что для большого графа Spark показал время в 5 раз лучшее, чем Hadoop MapReduce. 
Отчасти это объясняется in-memory обработкой данных в Spark, чего нет в Hadoop MapReduce.
*Что касается работы с API, то Spark в сравнении с Hadoop MapReduce предлагает более императивный стиль программирования.

